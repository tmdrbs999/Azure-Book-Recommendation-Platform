import azure.functions as func
import logging
import requests
import pandas as pd
import json
import os
import io
from azure.storage.blob import BlobServiceClient
from azure.eventhub import EventHubProducerClient, EventData
from datetime import datetime
import time
import numpy as np
import re
from pytz import timezone


# # í•œêµ­ ì‹œê°„ëŒ€ë¡œ í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
# now_korea = datetime.now(timezone('Asia/Seoul'))

app = func.FunctionApp()  # âœ… ìµœì‹  êµ¬ì¡°ì—ì„œ í•„ìˆ˜

# í™˜ê²½ ë³€ìˆ˜ (local.settings.json ë˜ëŠ” Application Settings)
API_KEY = os.getenv("API_KEY")
BASE_URL = "https://openapi.gg.go.kr/GGJOBABARECRUSTM"
STORAGE_CONN_STR = os.getenv("AzureWebJobsStorage")
EVENTHUB_CONN_STR = os.getenv("EVENTHUB_CONN_STR")
# EVENTHUB_NAME = os.getenv("EVENTHUB_NAME", "events-job")
EVENTHUB_NAME = os.getenv("EVENTHUB_NAME", "2dt-1st-team1-event-ggjob")


# ================================================
# ì „ì²˜ë¦¬ í•¨ìˆ˜
# ================================================

# ê¸‰ì—¬ì¡°ê±´ ë¶„ë¦¬
def parse_salary(salary_text: str):
    # ì•„ì˜ˆ ë¹„ì–´ìˆëŠ” ê²½ìš° ê³µê³ í™•ì¸
    if pd.isna(salary_text):
        return pd.Series([None, "ê³µê³ í™•ì¸"])

    text = str(salary_text).strip()

    # 1ï¸âƒ£ ë‹¨ìœ„ ì¸ì‹
    if "ì‹œê¸‰" in text:
        unit = "ì‹œê¸‰"
    elif "ì¼ê¸‰" in text:
        unit = "ì¼ê¸‰"
    elif "ì›”ê¸‰" in text:
        unit = "ì›”ê¸‰"
    elif "ì—°ë´‰" in text:
        unit = "ì—°ë´‰"
    elif "ë‚´ê·œ" in text:
        unit = "ë‚´ê·œ"
    else:
        unit = "ì—°ë´‰"
        # ì¡°ê±´ ì¶”ê°€í•´ì•¼í•  ìˆ˜ë„?

    # 2ï¸âƒ£ ìˆ«ì ì¶”ì¶œ
    nums = re.findall(r"\d+", text)
    nums = [int(n) for n in nums]

    if not nums:
        # ìˆ«ìê°€ ì—†ëŠ” ê²½ìš° (íšŒì‚¬ë‚´ê·œ ë“±)
        return pd.Series([np.nan, unit])

    # 3ï¸âƒ£ ê¸ˆì•¡ ê³„ì‚° ë¡œì§
    if "~" in text:
        # ë²”ìœ„ì¸ ê²½ìš° -> í‰ê· ê°’(ì¤‘ìœ„ê°’)
        value = np.mean(nums)
    elif "ì´í•˜" in text:
        # ì´í•˜ -> ìµœëŒ€ê°’
        value = max(nums)
    elif "ì´ìƒ" in text or "ì´ˆê³¼" in text:
        # ì´ìƒ/ì´ˆê³¼ -> ìµœì†Œê°’
        value = min(nums)
    else:
        # ë‹¨ì¼ ê¸ˆì•¡
        value = nums[0]

    # # 4ï¸âƒ£ ë‹¨ìœ„ ë³€í™˜ (ì› -> ë§Œì›)
    # if "ì›" in text and "ë§Œì›" not in text:
    #     value = value / 10000  # ì› -> ë§Œì›

    # 4ï¸âƒ£ ë‹¨ìœ„ ë³€í™˜ (ë§Œì› -> ì›)
    if "ë§Œì›" in text:
        value = value * 10000  # ì› -> ë§Œì›

    return pd.Series([round(value, 1), unit])


# ê·¼ë¬´ì§€ì—­ ë¶„ë¦¬, "ê²½ê¸°" ì‚½ì…
def split_region(region_text):
    # None ë˜ëŠ” NaN ì²˜ë¦¬
    if pd.isna(region_text) or str(region_text).strip().lower() == "none":
        # region1~region5 ëª¨ë‘ Noneìœ¼ë¡œ ë°˜í™˜
        return pd.Series([None]*5, index=[f"REGION{i+1}" for i in range(5)])

    # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ -> ê³µë°± ì œê±°
    regions = [r.strip() for r in str(region_text).split(',') if r.strip()]

    processed = []
    for r in regions:
        if r.startswith(("ì „êµ­", "ì„œìš¸", "ì¸ì²œ", "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ëŒ€ì „", "ì„¸ì¢…"
                         , "ê²½ë¶", "ê²½ë‚¨", "ëŒ€êµ¬", "ë¶€ì‚°", "ìš¸ì‚°", "ì „ë¶", "ì „ë‚¨", "ê´‘ì£¼ê´‘", "ì œì£¼")) or r==None:
            processed.append(r)
        else:
            processed.append(f"ê²½ê¸° {r}")

    # ìµœëŒ€ 5ê°œê¹Œì§€ ë§ì¶”ê¸° (ë¶€ì¡±í•˜ë©´ None)
    while len(processed) < 5:
        processed.append(None)
    return pd.Series(processed[:5], index=[f"REGION{i+1}" for i in range(5)])



# ê·¼ë¬´ì§€ì—­ ë¶„ë¦¬ x, "ê²½ê¸°" ì‚½ì…
def add_gg_region(region_text):
    # None ë˜ëŠ” NaNì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if pd.isna(region_text) or str(region_text).strip().lower() == "none":
        return None

    # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬ í›„ ê³µë°± ì œê±°
    regions = [r.strip() for r in str(region_text).split(',') if r.strip()]

    # ê° ì§€ì—­ ì•ì— ì ‘ë‘ì–´ ì¶”ê°€
    processed = []
    for r in regions:
        if r.startswith(("ì „êµ­", "ì„œìš¸", "ì¸ì²œ", "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ëŒ€ì „", "ì„¸ì¢…"
                         , "ê²½ë¶", "ê²½ë‚¨", "ëŒ€êµ¬", "ë¶€ì‚°", "ìš¸ì‚°", "ì „ë¶", "ì „ë‚¨", "ê´‘ì£¼ê´‘", "ì œì£¼")) or r==None:
            processed.append(r)               # ì„œìš¸, ì¸ì²œ ë“±ì€ ê·¸ëŒ€ë¡œ
        else:
            processed.append(f"ê²½ê¸° {r}")     # ë‚˜ë¨¸ì§€ëŠ” 'ê²½ê¸° ' ë¶™ì´ê¸°

    # ë‹¤ì‹œ ì‰¼í‘œë¡œ ë¬¶ì–´ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜
    return ", ".join(processed)


#  í•™ë ¥ noneì¼ ê²½ìš° 0ìœ¼ë¡œ ì¼ê´„ ì±„ì›€
def acdmcr_nan(acdmcr_text):
    if acdmcr_text == None:
        return 0
    else: return acdmcr_text
    

# ê²½ë ¥êµ¬ë¶„ ë‹¨ìˆœí™” - 1: ë¬´ê´€, 2: ì‹ ì…, 3: ê²½ë ¥, 4: ì‹ ì…/ê²½ë ¥ -> 1, 2, 4: ì‹ ì…, 3: ê²½ë ¥
def career_NE(career_text):
    if pd.isna(career_text):
        return None

    s = str(career_text).strip()

    # ìˆ«ìí† í° ëª¨ë‘ ì¶”ì¶œ (ì˜ˆ: "03,04" -> ["03","04"])
    tokens = re.findall(r'\d+', s)

    # ì•ì˜ 0 ì œê±°í•˜ì—¬ ì •ê·œí™” (ì˜ˆ: "03" -> "3")
    codes = {str(int(t)) for t in tokens}

    if '3' in codes:
        return 'ê²½ë ¥'
    if {'1','2','4'} & codes:
        return 'ê²½ë ¥ ë¬´ê´€'
    return None


# ì§ì—…ì½”ë“œ ê³µë€ ì²˜ë¦¬
def recruit_na(recruit_text):
    if pd.isna(recruit_text):
        return '999999'
    else:
        return recruit_text


# ê²½ë ¥ì½”ë“œ 4ìë¦¬ë¡œ ìë¦„
def career_4(career_text):
    return career_text[:4]


# ê° ìœ í˜•(ì¼ê¸‰ ì›”ê¸‰ ì—°ë´‰)ë³„ ê¸‰ì—¬ê°’ì„ ì›”ê¸‰ìœ¼ë¡œ í™˜ì‚°
def cal_wage_value_monthly(value: int, unit: str):
    if unit == "ì‹œê¸‰":
        return str(value * 209)
    elif unit == "ì¼ê¸‰":
        return str(value * 20)
    elif unit == "ì›”ê¸‰":
        return str(value)
    elif unit == "ì—°ë´‰":
        return str(round(value/12, 2))
    else:
        return None



# ================================================
# ì „ì²˜ë¦¬ ì§„í–‰ë¶€
# ================================================
def preprocess_jobs(raw_jobs):
    df = pd.DataFrame(raw_jobs)

    df[["SALARY_KRW", "SALARY_UNIT"]] = df["SALARY_COND"].apply(parse_salary)       # ê¸‰ì—¬ì¡°ê±´ ë¶„ë¦¬
    df["ACDMCR_nonNULL"] = df["ACDMCR_CD_NM"].apply(acdmcr_nan)                     # í•™ë ¥ì¡°ê±´ ê³µë°± -> 0(í•™ë ¥ë¬´ê´€)
    df["CAREER_TYPE"] = df["CAREER_CD_NM"].apply(career_NE)                         # ê²½ë ¥êµ¬ë¶„ ë‹¨ìˆœí™” - 1: ë¬´ê´€, 2: ì‹ ì…, 3: ê²½ë ¥, 4: ì‹ ì…/ê²½ë ¥ -> 1, 2, 4: ì‹ ì…, 3: ê²½ë ¥
    df["RECRUT_FIELD_CD_NM_nonNA"] = df["RECRUT_FIELD_CD_NM"].apply(recruit_na)     # ì§ì—…ì½”ë“œ ê³µë€ -> 999999
    df["RECRUT_FIELD_CD_NM_4"] = df["RECRUT_FIELD_CD_NM_nonNA"].apply(career_4)     # ì§ì—…ì½”ë“œ 4ìë¦¬ë¡œ ìë¦„
    df["REGION_GG"] = df["WORK_REGION_CONT"].apply(add_gg_region)                   # ê·¼ë¬´ì§€ì—­ -> ë¶„ë¦¬x, ì•ì— 'ê²½ê¸°'ë§Œ ì‚½ì…
    region_cols = df["WORK_REGION_CONT"].apply(split_region)                        # ê·¼ë¬´ì§€ì—­ -> ë¶„ë¦¬, ì•ì— 'ê²½ê¸°'ë§Œ ì‚½ì…
    df = pd.concat([df, region_cols], axis=1)
    df["wage_value_monthly"]=df.apply(lambda row: cal_wage_value_monthly(row["SALARY_KRW"], row["SALARY_UNIT"]), axis=1)
    # df["upload_time"] = datetime.utcnow().isoformat()

    # ìµœì¢… ì¹¼ëŸ¼ ì„ íƒ
    # df_filtered = df[['ENTRPRS_NM', 'PBANC_CONT', 'SALARY_KRW', 'SALARY_UNIT', 
    #                   'REGION_GG', 'REGION1', 'REGION2', 'REGION3', 'REGION4', 'REGION5', 
    #                   'CAREER_CD_NM', 'CAREER_TYPE', 'ACDMCR_nonNULL', 'RECRUT_FIELD_CD_NM', 'RECRUT_FIELD_CD_NM_nonNA', 'RECRUT_FIELD_NM']]

    df_filtered = df[['ENTRPRS_NM', 'PBANC_CONT', 'SALARY_UNIT', 'SALARY_KRW', 
                      'REGION1', 'CAREER_TYPE', 'RECRUT_FIELD_CD_NM_4', 
                      'RECRUT_FIELD_NM', 'CAREER_CD_NM', 'ACDMCR_nonNULL', 'wage_value_monthly']]

    Index_df_filtered = ['company', 'job_title', 'wage_type', 'wage_value_krw', 
                    'region', 'career', 'RCRIT_JSSFC_CMMN_CODE_SE', 
                    'JOBCODE_NM', 'CAREER_CND_CMMN_CODE_SE', 'ACDMCR_CMMN_CODE_SE', 'wage_value_monthly']


    return df_filtered, Index_df_filtered
    # return df


# ================================================
# API í˜¸ì¶œ í•¨ìˆ˜(chunk size, )
# ================================================
def fetch_jobs(size: int, pageIdx: int):
    # all_rows = []
    page_idx = pageIdx
    PAGE_SIZE = size

    params = {
        "KEY": API_KEY,
        "Type": "json",
        "pIndex": page_idx,
        "pSize": PAGE_SIZE,
    }
    
    response = requests.get(BASE_URL, params=params)
    response.encoding = 'utf-8'

    data = response.json()
    rows = data["GGJOBABARECRUSTM"][1]["row"]  # ì‹¤ì œ ë°ì´í„° ìœ„ì¹˜

    # all_rows.extend(rows)
    # print(f"{page_idx}í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ: {len(rows)}ê°œ í•­ëª©")


    df = pd.DataFrame(rows)
    print(f"ì´ ìˆ˜ì§‘ ê±´ìˆ˜: {len(df)}")

    return df


# ================================================
# Blob ì €ì¥ - json / csv
# ================================================
def save_to_blob(df):
    # í•œêµ­ ì‹œê°„ëŒ€ë¡œ í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    now_korea = datetime.now(timezone('Asia/Seoul'))

    blob_service = BlobServiceClient.from_connection_string(STORAGE_CONN_STR)
    container_client = blob_service.get_container_client("ggjob-data")
    filename = f"ggjobs_{now_korea.strftime('%Y%m%d_%H%M%S')}.json"
    # filename = f"jobs_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"

    json_bytes = df.to_json(orient="records", force_ascii=False).encode('utf-8')
    container_client.upload_blob(name=filename, data=io.BytesIO(json_bytes), overwrite=True)
    return filename


def save_to_blob_csv(df, df_header):
    now_korea = datetime.now(timezone('Asia/Seoul'))

    blob_service = BlobServiceClient.from_connection_string(STORAGE_CONN_STR)
    container_client = blob_service.get_container_client("ggjob-data")

    filename = f"ggjobs_{now_korea.strftime('%Y%m%d_%H%M%S')}.csv"
    blob_client = container_client.get_blob_client(filename)
    csv_bytes = df.to_csv(index=False, header=df_header, encoding="utf-8-sig").encode("utf-8-sig")
    blob_client.upload_blob(csv_bytes, overwrite=True)
    logging.info(f"Blob ì—…ë¡œë“œ ì™„ë£Œ: {filename}")

    return filename


# ================================================
# Blobì„ ì´ìš©í•´ í˜„ì¬ í˜ì´ì§€ ìƒíƒœë¥¼ ê´€ë¦¬
# ================================================
def get_next_page_from_blob(reset: bool = False) -> int:
    """í˜ì´ì§€ ìƒíƒœë¥¼ Azure Blob Storageì— ì €ì¥ ë° ê´€ë¦¬"""
    connection_str = os.environ["AzureWebJobsStorage"]
    container_name = "function-state"
    blob_name = "page_state.txt"

    blob_service = BlobServiceClient.from_connection_string(connection_str)
    container_client = blob_service.get_container_client(container_name)
    blob_client = container_client.get_blob_client(blob_name)

    # ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ìƒì„±
    try:
        container_client.create_container()
    except Exception:
        pass  # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ

    # === âœ… ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì²˜ë¦¬ ===
    if reset:
        blob_client.upload_blob("1", overwrite=True)
        logging.info("[RESET] í•¨ìˆ˜ ì¬ì‹œì‘ ê°ì§€ â†’ Blob ë‚´ page_state.txtë¥¼ 1ë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")
        return 1

    # === ê¸°ì¡´ í˜ì´ì§€ ìƒíƒœ ë¶ˆëŸ¬ì˜¤ê¸° ===
    try:
        data = blob_client.download_blob().readall().decode("utf-8").strip()
        last_page = int(data)
    except Exception:
        last_page = 1  # ì²˜ìŒ ì‹¤í–‰ ì‹œ 1ë¶€í„° ì‹œì‘

    # === í˜ì´ì§€ ì¦ê°€ í›„ ì €ì¥ ===
    next_page = last_page + 1
    blob_client.upload_blob(str(next_page), overwrite=True)
    return next_page


# ================================================
# Event Hubs ì „ì†¡
# ================================================
def send_to_eventhub(df, page_index: int):
    """
    ë§¤ ë¶„ë§ˆë‹¤ ìƒì„±ëœ DataFrame ì „ì²´ë¥¼ CSV ë¬¸ìì—´ë¡œ ë³€í™˜í•´
    í•˜ë‚˜ì˜ Eventë¡œ Event Hubì— ì „ì†¡í•œë‹¤.
    """
    producer = EventHubProducerClient.from_connection_string(
        conn_str=EVENTHUB_CONN_STR,
        eventhub_name=EVENTHUB_NAME
    )

    # === 1ï¸âƒ£ DataFrame â†’ CSV ë¬¸ìì—´ ë³€í™˜ ===
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding="utf-8-sig")
    csv_string = csv_buffer.getvalue()

    # === 2ï¸âƒ£ CSV ë³¸ë¬¸ ì•ì— ë©”íƒ€ë°ì´í„° ì£¼ì„ ì¶”ê°€ ===
    # header_line = f"# next_page_index: {page_index}, last_updated: {datetime.utcnow().isoformat()}\n"
    # event_body = header_line + csv_string

    # === 3ï¸âƒ£ EventData ìƒì„± ===
    event = EventData(csv_string)

    # === 4ï¸âƒ£ Event Hubë¡œ ì „ì†¡ ===
    with producer:
        batch = producer.create_batch()
        batch.add(event)
        producer.send_batch(batch)

    logging.info(f"âœ… EventHub ì „ì†¡ ì™„ë£Œ | í˜ì´ì§€ {page_index} | {len(df)}ê±´ | {len(csv_string)} bytes")


# ================================================
# HTTP Trigger (main)
# ================================================
# "0 0 * * * *" â†’ ë§¤ì‹œê°„ ì •ê°(ë¶„, ì´ˆ, ì¼, ì›”, ìš”ì¼ ë‹¨ìœ„)ë§ˆë‹¤ íŒŒì¼ ì—…ë¡œë“œ
# ë§¤ 30ë¶„ë§ˆë‹¤ë¼ë©´ "0 */30 * * * *"
@app.schedule(schedule="0 */1 * * * *", 
              arg_name="mytimer", 
              run_on_startup=False, 
              use_monitor=True)
def trig_connect_ggjobs(mytimer: func.TimerRequest):
    try:
        # âœ… í•¨ìˆ˜ ì²˜ìŒ ì‹œì‘(run_on_startup ì‹¤í–‰ ì‹œ) â†’ reset=Trueë¡œ ì´ˆê¸°í™”
        # ì´í›„ ë§¤ ë¶„ ì£¼ê¸° ì‹¤í–‰ì—ì„œëŠ” reset=Falseë¡œ ë™ì‘
        reset_flag = getattr(trig_connect_ggjobs, "_initialized", False) is False
        trig_connect_ggjobs._initialized = True

        page = get_next_page_from_blob(reset=reset_flag)
        logging.info(f"API í˜¸ì¶œ ì¤‘... (í˜ì´ì§€ {page})")

        # API ìš”ì²­
        raw_jobs = fetch_jobs(100, page)
        logging.info(f"ì´ {len(raw_jobs)}ê°œ ë°ì´í„° ìˆ˜ì§‘ (í˜ì´ì§€ {page})")

        # ë°ì´í„° ì „ì²˜ë¦¬
        logging.info("ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        df, header = preprocess_jobs(raw_jobs)

        # Blob ì €ì¥
        logging.info("Blob ì €ì¥ ì¤‘...")
        filename = save_to_blob_csv(df, header)

        # # Event Hubs ì „ì†¡
        # logging.info("Event Hub ì „ì†¡ ì¤‘...")
        # send_to_eventhub(df, page)

        logging.info(f"ì„±ê³µì ìœ¼ë¡œ {len(df)}ê±´ ì²˜ë¦¬ ì™„ë£Œ | Blob íŒŒì¼: {filename}")

    except Exception as e:
        logging.exception("ì—ëŸ¬ ë°œìƒ")



# ================================================
# Blob Trigger (CSV â†’ EventHubë¡œ ê·¸ëŒ€ë¡œ ì „ì†¡)
# ================================================
@app.blob_trigger(arg_name="myblob",
                  path="ggjob-data/{name}",
                  connection="AzureWebJobsStorage")
def blob_to_asa(myblob: func.InputStream):
    from azure.eventhub import EventHubProducerClient, EventData

    logging.info(f"Blob Trigger ì‹¤í–‰ë¨: {myblob.name} ({myblob.length} bytes)")

    # ğŸ”¥ JSON ë“± CSVê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ!
    if not myblob.name.lower().endswith(".csv"):
        logging.info(f"âš ï¸ CSV íŒŒì¼ì´ ì•„ë‹ˆë¼ ë¬´ì‹œí•©ë‹ˆë‹¤: {myblob.name}")
        return

    try:
        blob_bytes = myblob.read()
        blob_str = blob_bytes.decode('utf-8-sig')  # BOM ì œê±°

        logging.info("CSV ì›ë³¸ ì½ê¸° ì™„ë£Œ")

        producer = EventHubProducerClient.from_connection_string(
            conn_str=os.getenv("EVENTHUB_CONN_STR"),
            eventhub_name=os.getenv("EVENTHUB_NAME")
        )

        with producer:
            batch = producer.create_batch()
            batch.add(EventData(blob_str))
            producer.send_batch(batch)

        logging.info(f"CSV íŒŒì¼ {myblob.name} EventHubë¡œ ì „ì†¡ ì™„ë£Œ")

    except Exception as e:
        logging.exception(f"Blob ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")



# ==========================
# Blob Trigger
# ==========================
# @app.blob_trigger(arg_name="myblob",
#                   path="ggjob-data/{name}",
#                   connection="AzureWebJobsStorage")
# def blob_to_asa(myblob: func.InputStream):
#     import json
#     from azure.eventhub import EventHubProducerClient, EventData

#     logging.info(f"Blob Trigger ì‹¤í–‰ë¨: {myblob.name} ({myblob.length} bytes)")

#     try:
#         # 1ï¸âƒ£ Blob ë°ì´í„° ì½ê¸°
#         blob_data = myblob.read().decode('utf-8-sig')
#         logging.info("Blob ë°ì´í„° ì½ê¸° ì™„ë£Œ")

#         # 2ï¸âƒ£ JSON íŒŒì‹± (í•„ìš”ì‹œ ì „ì²˜ë¦¬)
#         # records = json.loads(blob_data)
#         records = pd.read_csv(io.StringIO(blob_data))

#         # 3ï¸âƒ£ Event Hubsë¡œ ì „ì†¡ â†’ Stream Analyticsê°€ ìˆ˜ì‹ í•˜ë„ë¡
#         producer = EventHubProducerClient.from_connection_string(
#             conn_str=os.getenv("EVENTHUB_CONN_STR"),
#             eventhub_name=os.getenv("EVENTHUB_NAME")
#         )

#         with producer:
#             batch = producer.create_batch()
#             for record in records:
#                 # ê° í–‰ì„ í•˜ë‚˜ì˜ EventDataë¡œ ì „ì†¡
#                 batch.add(EventData(json.dumps(record, ensure_ascii=False)))
#             producer.send_batch(batch)

#         logging.info(f"Blob {myblob.name} ì²˜ë¦¬ ì™„ë£Œ, {len(records)}ê±´ ì „ì†¡")

#     except Exception as e:
#         logging.exception(f"Blob ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")