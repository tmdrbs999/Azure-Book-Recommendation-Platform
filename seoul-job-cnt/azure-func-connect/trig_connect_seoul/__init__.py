import logging
import azure.functions as func
import requests
import pandas as pd
from datetime import datetime
from azure.storage.blob import BlobServiceClient
import os
import re
import tempfile
import json
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter


# === í™˜ê²½ ì„¤ì • ìƒìˆ˜ ===
STATE_BLOB_NAME = "state/current_start_index.json" # í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•  Blob íŒŒì¼ ê²½ë¡œ
CHUNK_SIZE = 100 # í•œ ë²ˆì˜ í•¨ìˆ˜ ì‹¤í–‰(1ë¶„) ì‹œ ê°€ì ¸ì˜¬ ë ˆì½”ë“œ ìˆ˜ <-- ìˆ˜ì •ë¨ (100)
DEFAULT_START_INDEX = 1 # ì‹œì‘ ì¸ë±ìŠ¤ (APIì˜ ì²« í˜ì´ì§€)

# =========================================================================
# === 1. Session ìƒì„± í•¨ìˆ˜ (API ì¬ì‹œë„ ë¡œì§) ===
# =========================================================================
def build_session(total_retries: int = 3, backoff: float = 1.0) -> requests.Session:
    """HTTP ìš”ì²­ ì„¸ì…˜ì„ ì„¤ì •í•˜ê³  ì¬ì‹œë„ ì •ì±…ì„ ì ìš©í•©ë‹ˆë‹¤."""
    s = requests.Session()
    # 429(Rate Limit), 5xx ì„œë²„ ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„í•˜ë„ë¡ ì„¤ì •
    retries = Retry(total=total_retries, backoff_factor=backoff, status_forcelist=[429, 500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    s.mount('https://', adapter)
    s.mount('http://', adapter)
    return s


# =========================================================================
# === 2. JSON/í…ìŠ¤íŠ¸ íŒŒì‹± ìœ í‹¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ===
# =========================================================================
def extract_by_path(obj, path: str):
    """JSON ê°ì²´ì—ì„œ '.' ê²½ë¡œë¥¼ ì´ìš©í•´ ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not path:
        return obj
    cur = obj
    for p in path.split('.'):
        if isinstance(cur, dict) and p in cur:
            cur = cur[p]
        else:
            return None
    return cur

def ensure_list(x):
    """ì…ë ¥ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if x is None:
        return []
    if isinstance(x, list):
        return x
    return [x]

def parse_wage(text):
    """ì‹œê¸‰/ì›”ê¸‰ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ê¸ˆì•¡(KRW)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # (ì›ë˜ì˜ ìƒì„¸í•œ íŒŒì‹± ë¡œì§ ìœ ì§€)
    if not isinstance(text, str):
        return {'wage_type': None, 'wage_value_krw': None, 'wage_raw': text}
    s = text.strip()
    m = re.search(r'\(?(ì›”ê¸‰|ì‹œê¸‰)\)?\s*[/\\]?\s*([0-9,\.]+)\s*(ë§Œì›|ì›)?', s)
    if m:
        wtype, num, unit = m.group(1), m.group(2), m.group(3) or 'ì›'
        try:
            num_val = int(float(num.replace(',', '')))
        except Exception:
            num_val = None
        value = num_val * 10000 if unit == 'ë§Œì›' else num_val
        return {'wage_type': wtype, 'wage_value_krw': value, 'wage_raw': text}
    m2 = re.search(r'([0-9,\.]+)\s*(ë§Œì›|ì›)', s)
    if m2:
        try:
            num_val = int(float(m2.group(1).replace(',', '')))
        except Exception:
            num_val = None
        unit = m2.group(2)
        value = num_val * 10000 if unit == 'ë§Œì›' else num_val
        wtype = 'ì›”ê¸‰' if 'ì›”' in s else ('ì‹œê¸‰' if 'ì‹œ' in s else None)
        return {'wage_type': wtype, 'wage_value_krw': value, 'wage_raw': text}
    return {'wage_type': None, 'wage_value_krw': None, 'wage_raw': text}

def parse_gui_ln(gui):
    """GUI_LN ë¬¸ìì—´ì—ì„œ ì§€ì—­(region)ê³¼ ê²½ë ¥(career)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # (ì›ë˜ì˜ ìƒì„¸í•œ íŒŒì‹± ë¡œì§ ìœ ì§€)
    if not isinstance(gui, str):
        return {'region': None, 'career': None, 'gui_raw': gui}
    parts = [p.strip() for p in gui.split('/')]
    region = parts[1] if len(parts) >= 2 else None
    career = parts[2] if len(parts) >= 3 else None
    return {'region': region, 'career': career, 'gui_raw': gui}


# =========================================================================
# === 3. ìƒíƒœ ê´€ë¦¬ (Load/Save Start Index) ===
# =========================================================================
def get_blob_client(conn_str: str, container_name: str, blob_name: str):
    """Blob Client ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    blob_service_client = BlobServiceClient.from_connection_string(conn_str)
    container_client = blob_service_client.get_container_client(container_name)
    try:
        container_client.create_container() # ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ìƒì„±
    except Exception:
        pass
    return container_client.get_blob_client(blob_name)

def load_start_index(blob_client):
    """Blob Storageì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì„±ê³µí•œ start_indexë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        download_stream = blob_client.download_blob()
        data = json.loads(download_stream.readall())
        start_index = data.get('next_start_index', DEFAULT_START_INDEX)
        logging.info(f"ğŸ’¾ ìƒíƒœ ë¡œë“œ ì„±ê³µ: ë‹¤ìŒ ì‹œì‘ ì¸ë±ìŠ¤ = {start_index}")
        return start_index
    except Exception as e:
        # íŒŒì¼ì´ ì—†ê±°ë‚˜(404) íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        logging.warning(f"âš ï¸ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” íŒŒì¼ ì—†ìŒ: {e}. ê¸°ë³¸ê°’ ({DEFAULT_START_INDEX})ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return DEFAULT_START_INDEX

def save_start_index(blob_client, next_start_index: int):
    """ë‹¤ìŒ í˜¸ì¶œì„ ìœ„í•œ start_indexë¥¼ Blob Storageì— ì €ì¥í•©ë‹ˆë‹¤."""
    state_data = {'next_start_index': next_start_index, 'last_updated': datetime.now().isoformat()}
    blob_client.upload_blob(json.dumps(state_data), overwrite=True)
    logging.info(f"ğŸ’¾ ìƒíƒœ ì €ì¥ ì„±ê³µ: ë‹¤ìŒ ì‹œì‘ ì¸ë±ìŠ¤ = {next_start_index}")


# =========================================================================
# === 4. ë‹¨ì¼ ì²­í¬ API í˜¸ì¶œ (Industry ì½”ë“œ ì œê±°) ===
# =========================================================================
# industry íŒŒë¼ë¯¸í„° ì œê±°
def fetch_one_chunk_of_jobs(session: requests.Session, api_key: str, start_index: int, chunk_size: int = CHUNK_SIZE):
    """ì§€ì •ëœ start_indexë¶€í„° chunk_sizeë§Œí¼ì˜ ë ˆì½”ë“œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    
    end_index = start_index + chunk_size - 1
    # URLì—ì„œ // ë‹¤ìŒì— ìˆë˜ {industry} ë¶€ë¶„ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
    url = f"http://openapi.seoul.go.kr:8088/{api_key}/json/GetJobInfo/{start_index}/{end_index}/" 
    
    logging.info(f"ğŸš€ API ìš”ì²­ ë²”ìœ„ (ì „ì²´ ì‚°ì—…): Start={start_index}, End={end_index}")

    try:
        resp = session.get(url, timeout=15)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        logging.error(f"âŒ API ìš”ì²­ ì‹¤íŒ¨ (Start={start_index}): {e}")
        return [], start_index # ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  ì¢…ë£Œ
    
    records = extract_by_path(data, "GetJobInfo.row")
    records = ensure_list(records)
    
    # ë‹¤ìŒ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    next_start_index = start_index + len(records)
    
    if not records:
        logging.info("â­ API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¼ì˜ ëì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return records, next_start_index


# =========================================================================
# === 5. ë°ì´í„° ì •ì œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ===
# =========================================================================
def clean_dataframe(df: pd.DataFrame, convert_monthly: bool = True, hours_per_month: int = 209) -> pd.DataFrame:
    """ë°ì´í„°í”„ë ˆì„ì„ ì •ì œí•˜ê³  ì„ê¸ˆ ì •ë³´ ë“±ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    # (ì›ë˜ì˜ ìƒì„¸í•œ ì •ì œ ë¡œì§ ìœ ì§€)
    keep = [
        'CMPNY_NM', 'JO_SJ', 'HOPE_WAGE', 'GUI_LN',
        'RCRIT_JSSFC_CMMN_CODE_SE', 'JOBCODE_NM', 'CAREER_CND_CMMN_CODE_SE', 'ACDMCR_CMMN_CODE_SE'
    ]
    existing = [c for c in keep if c in df.columns]
    out = df[existing].copy()
    out = out.rename(columns={
        'CMPNY_NM': 'company',
        'JO_SJ': 'job_title',
        'HOPE_WAGE': 'hope_wage',
        'GUI_LN': 'gui_ln'
    })

    wage_df = pd.DataFrame(out['hope_wage'].fillna('').apply(parse_wage).tolist(), index=out.index)
    gui_df = pd.DataFrame(out['gui_ln'].fillna('').apply(parse_gui_ln).tolist(), index=out.index)

    out = pd.concat([out, wage_df, gui_df], axis=1)

    if convert_monthly:
        def to_monthly(row):
            if row.get('wage_type') == 'ì‹œê¸‰' and row.get('wage_value_krw'):
                return int(row['wage_value_krw'] * hours_per_month)
            if row.get('wage_type') == 'ì›”ê¸‰' and row.get('wage_value_krw'):
                return int(row['wage_value_krw'])
            return None
        out['wage_value_monthly'] = out.apply(to_monthly, axis=1)

    # RCRIT_JSSFC_CMMN_CODE_SE ì»¬ëŸ¼ ì²˜ë¦¬
    def process_rcrit_code(code):
        if pd.isna(code) or code == '':
            return None
        code_str = str(code).strip()
        if code_str.isdigit():
            if len(code_str) == 5:
                code_str = '0' + code_str
            if len(code_str) > 2:
                code_str = code_str[:-2]
        return code_str

    if 'RCRIT_JSSFC_CMMN_CODE_SE' in out.columns:
        out['RCRIT_JSSFC_CMMN_CODE_SE'] = out['RCRIT_JSSFC_CMMN_CODE_SE'].apply(process_rcrit_code)

    # wage_type ì¶”ë¡ 
    def infer_wage_type(row):
        wt = row.get('wage_type')
        if wt is None or (isinstance(wt, float) and pd.isna(wt)) or (isinstance(wt, str) and wt.strip() == ''):
            v = row.get('wage_value_krw')
            try:
                vnum = int(float(v)) if v is not None else 0
            except Exception:
                vnum = 0
            if vnum // 1000000 == 0:
                return "ê³µê³  í™•ì¸"
            else:
                return "ì—°ë´‰"
        return wt

    out['wage_type'] = out.apply(infer_wage_type, axis=1)

    # ìµœì¢… í•„í„°ë§ ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê¸°
    filtered_cols = [
        'company', 'job_title', 'wage_type', 'wage_value_krw', 'region', 'career',
        'RCRIT_JSSFC_CMMN_CODE_SE', 'JOBCODE_NM', 'CAREER_CND_CMMN_CODE_SE', 'ACDMCR_CMMN_CODE_SE',
        'wage_value_monthly'
    ]
    for c in filtered_cols:
        if c not in out.columns:
            out[c] = None
    
    return out[filtered_cols].copy()


# =========================================================================
# === 6. Azure Function Main (Timer Trigger) (Industry ì½”ë“œ ì œê±°) ===
# =========================================================================
def main(mytimer: func.TimerRequest) -> None:
    """1ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” íƒ€ì´ë¨¸ íŠ¸ë¦¬ê±° ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    utc_timestamp = datetime.utcnow().isoformat()
    logging.info(f'Python Timer Trigger ì‹œì‘: {utc_timestamp}')
    
    # API ìš”ì²­ URL êµ¬ì„±ì— í•„ìš”í•˜ì§€ ì•Šì€ industry ë³€ìˆ˜ ì„ ì–¸/ê²€ì¦ ë¡œì§ ì‚­ì œ

    try:
        # (1) í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë¡œë“œ
        # industry ë³€ìˆ˜ ì‚­ì œ
        api_key = os.getenv("API_KEY", "ì¸ì¦í‚¤")
        blob_conn_str = os.getenv("AzureWebJobsStorage")
        container_name = os.getenv("BLOB_CONTAINER_NAME", "seoul-job-ct")
        
        if not blob_conn_str:
            logging.error("âŒ AzureWebJobsStorage ì—°ê²° ë¬¸ìì—´ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # (2) ìƒíƒœ ê´€ë¦¬ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° í˜„ì¬ ì‹œì‘ ì¸ë±ìŠ¤ ë¡œë“œ
        state_blob_client = get_blob_client(blob_conn_str, container_name, STATE_BLOB_NAME)
        current_start_index = load_start_index(state_blob_client)
        
        # (3) API í˜¸ì¶œ ì„¸ì…˜ ìƒì„±
        session = build_session()
        
        # (4) ë‹¨ì¼ ì²­í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (100ê±´)
        # fetch_one_chunk_of_jobs í˜¸ì¶œ ì‹œ industry ì¸ìˆ˜ë¥¼ ì œê±°í–ˆìŠµë‹ˆë‹¤.
        records, next_start_index = fetch_one_chunk_of_jobs(
            session, api_key, current_start_index, CHUNK_SIZE
        )

        if not records:
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  (ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´) ì¢…ë£Œ
            logging.info("â­ ì´ë²ˆ í˜¸ì¶œì—ì„œ ìƒˆ ë ˆì½”ë“œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # (5) ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ì œ
        df = pd.DataFrame(records)
        filtered_df = clean_dataframe(df)
        
        # (6) CSV ìƒì„± ë° Blob ì—…ë¡œë“œ (ìƒˆ íŒŒì¼ë¡œ ì €ì¥)
        # íŒŒì¼ ê²½ë¡œì—ì„œ industry í´ë”ëª… ëŒ€ì‹  'all' ë˜ëŠ” í˜„ì¬ëŠ” ë¹ˆ ë¬¸ìì—´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # ë°ì´í„°ê°€ í•„í„°ë§ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ 'all'ì„ ì‚¬ìš©í•˜ê±°ë‚˜, íŒŒì¼ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” íŒŒì¼ëª… ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ 'all_jobs' í´ë”ë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
        file_name = f"data/all_jobs/seoul_jobs_{current_start_index}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        output_blob_client = get_blob_client(blob_conn_str, container_name, file_name)
        
        # CSV ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ Blobìœ¼ë¡œ ì—…ë¡œë“œ
        csv_bytes = filtered_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        output_blob_client.upload_blob(csv_bytes, overwrite=True)
        logging.info(f"âœ… Blob ì—…ë¡œë“œ ì™„ë£Œ: {file_name} ({len(filtered_df)}ê±´)")

        # (7) ë‹¤ìŒ ì‹œì‘ ì¸ë±ìŠ¤ ì €ì¥ (ì„±ê³µì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì €ì¥í•œ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸)
        save_start_index(state_blob_client, next_start_index)
        
    except Exception as e:
        logging.error(f"âŒ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

    logging.info('Python Timer Trigger ì™„ë£Œ.')